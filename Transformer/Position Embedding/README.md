# Position Embedding

Position Encoding: inject some information about the relative or absolute position of the tokens in the sequence.

Positional Embedding: add to the input embeddings at the bottoms of the encoder and decoder stacks, having the same dimension $d_{model}$ as the embeddings.
